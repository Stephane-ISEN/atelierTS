{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ca4f47b",
   "metadata": {},
   "source": [
    "##############################################################################\n",
    "##############################################################################\n",
    "### **Atelier \"Faire du Machine Learning et du Deep Learning sur des Time Series\"**\n",
    "##############################################################################\n",
    "##############################################################################\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "#### **Partie 3 : Détection d'anomalies sur des séries temporelles et explicabilité sur ces anomalies**\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Dans ce dernier notebook, nous allons utiliser la méthode de l'isolation forest pour détecter des anomalies sur nos time series. Nous allons aussi utiliser SHAP pour avoir des indices sur les caractéristiques qui expliquent le classement d'un sample en anomalie.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea10058",
   "metadata": {},
   "source": [
    "## Section 0 : récupération, préparation et Fusion des Datasets ##\n",
    "\n",
    "L'analyse multivariée repose sur l'intégration de variables exogènes (externes) pour améliorer la précision prédictive. Ici, nous fusionnons les données de consommation électrique avec les données météorologiques.\n",
    "\n",
    "    Alignement temporel : Synchronisation des deux sources sur un index commun via une jointure interne (inner merge).\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ba5c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def fetch_brest_electricity_data():\n",
    "    base_url = \"https://odre.opendatasoft.com/api/explore/v2.1/catalog/datasets/eco2mix-metropoles-tr/exports/json\"\n",
    "    \n",
    "    # Paramètres de la requête\n",
    "    params = {\n",
    "        \"where\": \"libelle_metropole='Brest Métropole' AND date_heure >= '2020-01-01' AND date_heure <= '2025-12-31'\",\n",
    "        \"order_by\": \"date_heure ASC\",\n",
    "        \"timezone\": \"UTC\"\n",
    "    }\n",
    "    \n",
    "    print(\"Récupération des données pour Brest Métropole (2020-2025)...\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        df = pd.DataFrame(data)\n",
    "        print(df.head())\n",
    "            \n",
    "        df = df[['date_heure', 'consommation']].dropna()\n",
    "        \n",
    "        print(f\"Chargement Terminé ! {len(df)} lignes récupérées.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la récupération : {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "df_conso_brest = fetch_brest_electricity_data()\n",
    "\n",
    "if df_conso_brest is not None:\n",
    "    print(df_conso_brest.head())\n",
    "    # Sauvegarde pour l'atelier\n",
    "    df_conso_brest.to_csv(\"conso_brest_2020_2025.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3a737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 1. indexation temporelle\n",
    "df_conso_brest = df_conso_brest.rename(columns={\"date_heure\": \"date\"})\n",
    "df_conso_brest['date'] = pd.to_datetime(df_conso_brest['date'], utc=True)\n",
    "df_conso_brest = df_conso_brest.set_index('date').sort_index()\n",
    "\n",
    "# changement de temporalité des données\n",
    "df_conso_brest_journ = df_conso_brest['consommation'].resample('D').mean().interpolate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0cb370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "import pandas as pd\n",
    "from retry_requests import retry\n",
    "\n",
    "# Configuration de l'API avec cache et relance automatique en cas d'erreur\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=-1)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "def get_weather_data_brest(start_date, end_date):\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    \n",
    "    params = {\n",
    "        \"latitude\": 48.3904, # Brest\n",
    "        \"longitude\": -4.4861,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"hourly\": [\"temperature_2m\", \"relative_humidity_2m\", \"wind_speed_10m\", \"shortwave_radiation\"],\n",
    "        \"timezone\": \"Europe/Berlin\"\n",
    "    }\n",
    "    \n",
    "    responses = openmeteo.weather_api(url, params=params)\n",
    "    response = responses[0]\n",
    "\n",
    "    # Processus de transformation des données horaires\n",
    "    hourly = response.Hourly()\n",
    "    hourly_data = {\"date\": pd.date_range(\n",
    "        start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "        end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "        freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "        inclusive=\"left\"\n",
    "    )}\n",
    "    \n",
    "    hourly_data[\"temp_moy\"] = hourly.Variables(0).ValuesAsNumpy()\n",
    "    hourly_data[\"humidity\"] = hourly.Variables(1).ValuesAsNumpy()\n",
    "    hourly_data[\"vent_vitesse\"] = hourly.Variables(2).ValuesAsNumpy()\n",
    "    hourly_data[\"rayonnement_moyen\"] = hourly.Variables(3).ValuesAsNumpy()\n",
    "\n",
    "    df_meteo = pd.DataFrame(data=hourly_data)\n",
    "    \n",
    "    # Passage en format journalier\n",
    "    df_meteo_journ = df_meteo.resample('D', on='date').mean()\n",
    "    \n",
    "    return df_meteo_journ\n",
    "\n",
    "\n",
    "df_meteo_brest_journ = get_weather_data_brest(\"2020-01-01\", \"2025-12-31\")\n",
    "print(df_meteo_brest_journ.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f359f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fusion (Jointure sur la date)\n",
    "\n",
    "df_data_multi_brest = pd.merge(df_conso_brest_journ, df_meteo_brest_journ, on='date', how='inner')\n",
    "\n",
    "print(df_data_multi_brest.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd7bf50",
   "metadata": {},
   "source": [
    "## Section 1 : Ingénierie des caractéristiques ##\n",
    "\n",
    "Nous allons aider le modèle en construisant des caractéristiques reflétant la position du chargé de prédiction le jour j.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0226f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PASSÉ (Lags) ---\n",
    "# Consommation de la veille (J-1), de l'avant veille (J-2) et de la semaine dernière (J-7) pour la saisonnalité\n",
    "df_data_multi_brest['conso_obs_j-1'] = df_data_multi_brest['consommation'].shift(1)\n",
    "df_data_multi_brest['conso_obs_j-2'] = df_data_multi_brest['consommation'].shift(2)\n",
    "df_data_multi_brest['conso_obs_j-7'] = df_data_multi_brest['consommation'].shift(7)\n",
    "\n",
    "# Météo observée hier (J-1) pour l'inertie thermique\n",
    "df_data_multi_brest['temp_obs_j-1'] = df_data_multi_brest['temp_moy'].shift(1)\n",
    "\n",
    "# Météo observée hier (J-1) pour l'humidité\n",
    "df_data_multi_brest['humidity_j-1'] = df_data_multi_brest['humidity'].shift(1)\n",
    "\n",
    "# Météo observée hier (J-1) pour le rayonnement\n",
    "df_data_multi_brest['rayonnement_moyen_j-1'] = df_data_multi_brest['rayonnement_moyen'].shift(1)\n",
    "\n",
    "# Météo observée hier (J-1) pour le vent\n",
    "df_data_multi_brest['vent_vitesse_j-1'] = df_data_multi_brest['vent_vitesse'].shift(1)\n",
    "\n",
    "# --- FUTUR / PRÉVISIONS (Leads) ---\n",
    "# On utilise la donnée réelle de J comme si c'était la prévision faite le matin même\n",
    "df_data_multi_brest['temp_prev_j'] = df_data_multi_brest['temp_moy'] \n",
    "\n",
    "# On utilise la donnée de J+1 comme prévision pour demain (Lead)\n",
    "df_data_multi_brest['temp_prev_j+1'] = df_data_multi_brest['temp_moy'].shift(-1)\n",
    "\n",
    "\n",
    "df_data_multi_brest = df_data_multi_brest.dropna()\n",
    "\n",
    "print(df_data_multi_brest.head())\n",
    "\n",
    "\n",
    "# Définition des périodes\n",
    "dataset_train = df_data_multi_brest['2015-01-01':'2024-12-31']\n",
    "dataset_test = df_data_multi_brest['2025-01-01':'2025-12-31']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa2eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "\n",
    "# On définit les jours fériés en France \n",
    "fr_holidays = holidays.France()\n",
    "\n",
    "def add_calendar_features(df):\n",
    "    # On travaille sur une copie pour ne pas modifier l'original par mégarde\n",
    "    df_enriched = df.copy()\n",
    "    \n",
    "    # 1. Variables temporelles basiques\n",
    "    df_enriched['day_of_week'] = df_enriched.index.dayofweek\n",
    "    df_enriched['month'] = df_enriched.index.month\n",
    "    df_enriched['is_weekend'] = df_enriched.index.dayofweek.isin([5, 6]).astype(int)\n",
    "    \n",
    "    # 2. Jours fériés (Boolean : 1 si férié, 0 sinon)\n",
    "    df_enriched['is_holiday'] = df_enriched.index.map(lambda x: 1 if x in fr_holidays else 0)\n",
    "    \n",
    "    # 3.  Veille et Lendemain de jour férié \n",
    "    df_enriched['is_holiday_prev'] = df_enriched['is_holiday'].shift(-1, fill_value=0)\n",
    "    df_enriched['is_holiday_next'] = df_enriched['is_holiday'].shift(1, fill_value=0)\n",
    "    \n",
    "    return df_enriched\n",
    "\n",
    "# Application sur vos datasets\n",
    "dataset_train_enriched = add_calendar_features(dataset_train)\n",
    "dataset_test_enriched = add_calendar_features(dataset_test)\n",
    "\n",
    "print(dataset_test_enriched.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f849901c",
   "metadata": {},
   "source": [
    "## Section 2 : Détection des anomalies sur la consommation électrique de 2025 à Brest ##\n",
    "\n",
    "Nous allons maintenant utiliser la méthode de la forêt isolée pour détecter les anomalies de consommation.\n",
    "<br><br>\n",
    "L'idée sous-jacente de cette approche est de mesurer pour chaque échantillon combien de profondeur d'arbre de décision faut-il pour l'isoler de toute le reste du dataset. \n",
    "<br><br>\n",
    "L'idée : si cette profondeur est faible, ce sample est facilement séparable, il est une anomalie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091504a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Nettoyage des données\n",
    "train_clean = dataset_train_enriched.dropna()\n",
    "test_clean = dataset_test_enriched.dropna()\n",
    "\n",
    "# 2. Initialisation du modèle\n",
    "# n_estimators : nombre d'arbres \n",
    "# contamination : proportion estimée d'anomalies, sert à choisir le treshold détectant les anomalies \n",
    "model_if = IsolationForest(n_estimators=100, \n",
    "                           contamination=0.01)\n",
    "\n",
    "# 3. Entraînement de la foret sur le dataset d'entrainement\n",
    "#######################################\n",
    "#ligne à remplir\n",
    "#######################################\n",
    "model_if.fit()\n",
    "\n",
    "# 4. prédictions\n",
    "#######################################\n",
    "#ligne à remplir\n",
    "#######################################\n",
    "predictions = model_if.predict()\n",
    "\n",
    "\n",
    "test_clean['is_anomaly'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa320c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "# Tracer la consommation réelle\n",
    "plt.plot(test_clean.index, test_clean['consommation'], color='blue', alpha=0.4, label='Consommation')\n",
    "\n",
    "# Identifier les anomalies\n",
    "#######################################\n",
    "#ligne à remplir\n",
    "#######################################\n",
    "anomalies = test_clean[]\n",
    "\n",
    "# Superposer les points rouges\n",
    "plt.scatter(anomalies.index, anomalies['consommation'], color='red', label='Anomalie Détectée')\n",
    "\n",
    "plt.title('Détection d\\'événements rares via Isolation Forest')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81e6c7c",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "Nous avons bien détecté des anomalies en 2025. A vous de jouer, explorer ces anomalies et comprenez à quels jours particuliers de l'année sont-ils associés.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Cette analyse est intéressante, car nous comprendons rapidement la rareté évènementielle, mais qu'en est-il des paramètres qui expliquent ces anomalies ? est-ce des facteurs météo ou bien des facteurs calendaires ?\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c84d09",
   "metadata": {},
   "source": [
    "## Section 3 d'explicabilité : Montrer pourquoi une anomalie est une anomalie ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c9790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "# On s'assure que les données passées à SHAP sont des valeurs pures (numpy) \n",
    "# pour éviter les conflits de noms de colonnes ou d'index de Pandas\n",
    "features_cols = train_clean.columns.tolist()\n",
    "X_reference = train_clean[features_cols].values\n",
    "X_anomaly = test_clean[test_clean['is_anomaly'] == -1][features_cols].values\n",
    "print(test_clean[test_clean['is_anomaly'] == -1][features_cols])\n",
    "print(X_anomaly[0:1])\n",
    "\n",
    "\n",
    "# 1. Initialisation de l'explainer avec le modèle déjà entraîné\n",
    "# On utilise un échantillon du train pour la baseline\n",
    "explainer = shap.TreeExplainer(model_if, data=X_reference[:100])\n",
    "\n",
    "# 2. Calcul des SHAP values pour la première anomalie détectée\n",
    "#######################################\n",
    "#ligne à remplir\n",
    "#######################################\n",
    "shap_values_single = explainer.shap_values()\n",
    "\n",
    "# 3. Visualisation\n",
    "shap.force_plot(\n",
    "    explainer.expected_value, \n",
    "    shap_values_single[0], \n",
    "    features=X_anomaly[0:1],\n",
    "    #######################################\n",
    "    #ligne à remplir\n",
    "    ####################################### \n",
    "    feature_names=,\n",
    "    matplotlib=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2a255f",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "La visualisation n'est pas satisfaisante, on va plutot utiliser l'affichage en cascade\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dff542",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "explainer = shap.Explainer(model_if, train_clean)\n",
    "shap_values = explainer(test_clean[features_cols])\n",
    "\n",
    "# 2. On sélectionne l'index d'une anomalie détectée (ex: la première)\n",
    "#######################################\n",
    "#ligne à remplir\n",
    "#######################################\n",
    "anomalie_idx = test_clean[].index[0]\n",
    "# On récupère sa position entière dans le tableau pour SHAP\n",
    "loc_idx = test_clean.index.get_loc(anomalie_idx)\n",
    "\n",
    "# 3. Affichage en \"Waterfall\" (Cascade)\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.plots.waterfall(shap_values[loc_idx], max_display=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6ec0ef",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "A vous de jouer, expliquer pour chaque anomalie quels sont les facteurs prépondérants expliquant la surconsommation ou la sous-consommation. Est-ce cohérent à votre avis ?\n",
    "<br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
