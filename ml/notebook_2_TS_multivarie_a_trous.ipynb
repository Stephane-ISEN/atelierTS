{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47e52e76",
   "metadata": {},
   "source": [
    "##############################################################################\n",
    "##############################################################################\n",
    "### **Atelier \"Faire du Machine Learning et du Deep Learning sur des Time Series\"**\n",
    "##############################################################################\n",
    "##############################################################################\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "#### **Partie 2 : Machine Learning sur des Times Series multivari√©e** \n",
    "\n",
    "Dans cette deuxi√®me partie, nous allons ajouter des variables exog√®nes √† la s√©rie univari√©e historique de consommation d'√©lectricit√© de la m√©tropole de Brest. Nous allons rajouter tout d'abord des donn√©e m√©t√©orologiques, puis des donn√©es li√©es aux √©v√®nements calendaires.\n",
    "<br>\n",
    "<br>\n",
    "Niveau Mod√®les, nous allons entrainer un LSTM, qui est tout √† fait design√© pour g√©rer des s√©ries multivari√©es. La phase un peu compliqu√©e sera celle de la cr√©ation des s√©quences qui serviront de s√©quence historique pour une inf√©rence.\n",
    "<br>\n",
    "<br>\n",
    "En bonus, nous explorerons le premier mod√®le de fondation pour donn√©es multivari√©e, MOIRAI, qui est capable de faire des pr√©dictions en zero-shot learning. Nous verrons si cette approche permet d√©j√† des r√©sultats corrects\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea10058",
   "metadata": {},
   "source": [
    "## Section 0 : r√©cup√©ration, pr√©paration et Fusion des Datasets ##\n",
    "\n",
    "L'analyse multivari√©e repose sur l'int√©gration de variables exog√®nes (externes) pour am√©liorer la pr√©cision pr√©dictive. Ici, nous fusionnons les donn√©es de consommation √©lectrique avec les donn√©es m√©t√©orologiques.\n",
    "\n",
    "    Alignement temporel : Synchronisation des deux sources sur un index commun via une jointure interne (inner merge).\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ba5c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def fetch_brest_electricity_data():\n",
    "    base_url = \"https://odre.opendatasoft.com/api/explore/v2.1/catalog/datasets/eco2mix-metropoles-tr/exports/json\"\n",
    "    \n",
    "    # Param√®tres de la requ√™te\n",
    "    params = {\n",
    "        \"where\": \"libelle_metropole='Brest M√©tropole' AND date_heure >= '2020-01-01' AND date_heure <= '2025-12-31'\",\n",
    "        \"order_by\": \"date_heure ASC\",\n",
    "        \"timezone\": \"UTC\"\n",
    "    }\n",
    "    \n",
    "    print(\"R√©cup√©ration des donn√©es pour Brest M√©tropole (2020-2025)...\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        df = pd.DataFrame(data)\n",
    "        print(df.head())\n",
    "            \n",
    "        df = df[['date_heure', 'consommation']].dropna()\n",
    "        \n",
    "        print(f\"Chargement Termin√© ! {len(df)} lignes r√©cup√©r√©es.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la r√©cup√©ration : {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "df_conso_brest = fetch_brest_electricity_data()\n",
    "\n",
    "if df_conso_brest is not None:\n",
    "    print(df_conso_brest.head())\n",
    "    # Sauvegarde pour l'atelier\n",
    "    df_conso_brest.to_csv(\"conso_brest_2020_2025.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3a737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 1. indexation temporelle\n",
    "df_conso_brest = df_conso_brest.rename(columns={\"date_heure\": \"date\"})\n",
    "df_conso_brest['date'] = pd.to_datetime(df_conso_brest['date'], utc=True)\n",
    "df_conso_brest = df_conso_brest.set_index('date').sort_index()\n",
    "\n",
    "# changement de temporalit√© des donn√©es\n",
    "df_conso_brest_journ = df_conso_brest['consommation'].resample('D').mean().interpolate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0cb370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "import pandas as pd\n",
    "from retry_requests import retry\n",
    "\n",
    "# Configuration de l'API avec cache et relance automatique en cas d'erreur\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=-1)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "def get_weather_data_brest(start_date, end_date):\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    \n",
    "    params = {\n",
    "        \"latitude\": 48.3904, # Brest\n",
    "        \"longitude\": -4.4861,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"hourly\": [\"temperature_2m\", \"relative_humidity_2m\", \"wind_speed_10m\", \"shortwave_radiation\"],\n",
    "        \"timezone\": \"Europe/Berlin\"\n",
    "    }\n",
    "    \n",
    "    responses = openmeteo.weather_api(url, params=params)\n",
    "    response = responses[0]\n",
    "\n",
    "    # Processus de transformation des donn√©es horaires\n",
    "    hourly = response.Hourly()\n",
    "    hourly_data = {\"date\": pd.date_range(\n",
    "        start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "        end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "        freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "        inclusive=\"left\"\n",
    "    )}\n",
    "    \n",
    "    hourly_data[\"temp_moy\"] = hourly.Variables(0).ValuesAsNumpy()\n",
    "    hourly_data[\"humidity\"] = hourly.Variables(1).ValuesAsNumpy()\n",
    "    hourly_data[\"vent_vitesse\"] = hourly.Variables(2).ValuesAsNumpy()\n",
    "    hourly_data[\"rayonnement_moyen\"] = hourly.Variables(3).ValuesAsNumpy()\n",
    "\n",
    "    df_meteo = pd.DataFrame(data=hourly_data)\n",
    "    \n",
    "    # Passage en format journalier\n",
    "    df_meteo_journ = df_meteo.resample('D', on='date').mean()\n",
    "    \n",
    "    return df_meteo_journ\n",
    "\n",
    "\n",
    "df_meteo_brest_journ = get_weather_data_brest(\"2020-01-01\", \"2025-12-31\")\n",
    "print(df_meteo_brest_journ.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f359f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fusion (Jointure sur la date)\n",
    "\n",
    "df_data_multi_brest = pd.merge(df_conso_brest_journ, df_meteo_brest_journ, on='date', how='inner')\n",
    "\n",
    "print(df_data_multi_brest.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd7bf50",
   "metadata": {},
   "source": [
    "## Section 1  : Ing√©nierie des caract√©ristiques ##\n",
    "\n",
    "Nous allons aider le mod√®le en construisant des caract√©ristiques refl√©tant la position du charg√© de pr√©diction le jour j.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0226f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- PASS√â (Lags) ---\n",
    "# Consommation de la veille (J-1), de l'avant veille (J-2) et de la semaine derni√®re (J-7) pour la saisonnalit√©\n",
    "#######################################\n",
    "#lignes √† remplir\n",
    "#######################################\n",
    "df_data_multi_brest['conso_obs_j-1'] = df_data_multi_brest['consommation'].\n",
    "df_data_multi_brest['conso_obs_j-2'] = df_data_multi_brest['consommation'].\n",
    "df_data_multi_brest['conso_obs_j-7'] = df_data_multi_brest['consommation'].\n",
    "\n",
    "# M√©t√©o observ√©e hier (J-1) pour l'inertie thermique\n",
    "df_data_multi_brest['temp_obs_j-1'] = df_data_multi_brest['temp_moy'].\n",
    "\n",
    "# M√©t√©o observ√©e hier (J-1) pour l'humidit√©\n",
    "df_data_multi_brest['humidity_j-1'] = df_data_multi_brest['humidity'].\n",
    "\n",
    "# M√©t√©o observ√©e hier (J-1) pour le rayonnement\n",
    "df_data_multi_brest['rayonnement_moyen_j-1'] = df_data_multi_brest['rayonnement_moyen'].\n",
    "\n",
    "# M√©t√©o observ√©e hier (J-1) pour le vent\n",
    "df_data_multi_brest['vent_vitesse_j-1'] = df_data_multi_brest['vent_vitesse'].\n",
    "\n",
    "# --- FUTUR / PR√âVISIONS (Leads) ---\n",
    "# On utilise la donn√©e r√©elle de J comme si c'√©tait la pr√©vision faite le matin m√™me\n",
    "df_data_multi_brest['temp_prev_j'] = df_data_multi_brest['temp_moy'] \n",
    "\n",
    "# On utilise la donn√©e de J+1 comme pr√©vision pour demain (Lead)\n",
    "df_data_multi_brest['temp_prev_j+1'] = df_data_multi_brest['temp_moy'].\n",
    "\n",
    "\n",
    "df_data_multi_brest = df_data_multi_brest.dropna()\n",
    "\n",
    "print(df_data_multi_brest.head())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6d859b",
   "metadata": {},
   "source": [
    "Nous allons ajouter les informations calendaires comme caract√©ristiques potentiellement utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a022673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "\n",
    "fr_holidays = holidays.France()\n",
    "\n",
    "def add_calendar_features(df):\n",
    "    \n",
    "    df_enriched = df.copy()\n",
    "    \n",
    "    # 1. Variables temporelles basiques\n",
    "    df_enriched['day_of_week'] = df_enriched.index.dayofweek\n",
    "    df_enriched['month'] = df_enriched.index.month\n",
    "    df_enriched['is_weekend'] = df_enriched.index.dayofweek.isin([5, 6]).astype(int)\n",
    "    \n",
    "    # 2. Jours f√©ri√©s (Boolean : 1 si f√©ri√©, 0 sinon)\n",
    "    df_enriched['is_holiday'] = df_enriched.index.map(lambda x: 1 if x in fr_holidays else 0)\n",
    "    \n",
    "    # 3. Veille et Lendemain de jour f√©ri√© \n",
    "    df_enriched['is_holiday_prev'] = df_enriched['is_holiday'].shift(-1, fill_value=0)\n",
    "    df_enriched['is_holiday_next'] = df_enriched['is_holiday'].shift(1, fill_value=0)\n",
    "    \n",
    "    return df_enriched\n",
    "\n",
    "\n",
    "df_data_multi_brest = add_calendar_features(df_data_multi_brest)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49481d8",
   "metadata": {},
   "source": [
    "Enfin, nous allons s√©parer les donn√©es en donn√©es d'entrainement et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8277a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = df_data_multi_brest['2015-01-01':'2024-12-31']\n",
    "dataset_test = df_data_multi_brest['2025-01-01':'2025-12-31']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c17341d",
   "metadata": {},
   "source": [
    "## Section 2 : Normalisation des donn√©es (Scaling) ##\n",
    "\n",
    "Les mod√®les de Machine Learning (et particuli√®rement les r√©seaux de neurones ou SVR) sont sensibles √† l'√©chelle des donn√©es.\n",
    "\n",
    "    MinMaxScaler / StandardScaler : Transformation des valeurs pour les ramener dans un intervalle r√©duit (ex: [0,1]).\n",
    "\n",
    "    Objectif : √âviter que la variable \"Consommation\" (en centaines de MW) n'√©crase par exemple la variable \"Temp√©rature\" (en dizaines de degr√©s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeb7883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1. Normalisation\n",
    "scaler_uni = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_multi_x = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_multi_y = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# S√©lection des colonnes pour le mod√®le\n",
    "features = [\n",
    "    'conso_obs_j-1','conso_obs_j-2', 'conso_obs_j-7', \n",
    "    'temp_moy',   'humidity',  'vent_vitesse', 'rayonnement_moyen',\n",
    "    'temp_obs_j-1',  'humidity_j-1', 'rayonnement_moyen_j-1', 'rayonnement_moyen_j-1','vent_vitesse_j-1',\n",
    "    'temp_prev_j',  'temp_prev_j+1',\n",
    "    'day_of_week','month','is_weekend','is_holiday','is_holiday_prev','is_holiday_next'\n",
    "]\n",
    "target = 'consommation'\n",
    "\n",
    "X_train_multi = dataset_train[features].values\n",
    "y_train_multi = dataset_train[target].values\n",
    "\n",
    "X_test_multi = dataset_test[features].values\n",
    "y_test_multi = dataset_test[target].values\n",
    "\n",
    "X_train_multi_scaled = scaler_multi_x.fit_transform(X_train_multi)\n",
    "X_test_multi_scaled = scaler_multi_x.transform(X_test_multi)\n",
    "y_train_multi_scaled = scaler_multi_y.fit_transform(y_train_multi.reshape(-1, 1))\n",
    "y_test_multi_scaled = scaler_multi_y.transform(y_test_multi.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447a646a",
   "metadata": {},
   "source": [
    "Nous allons maintenant cr√©er les s√©quences qui serviront au LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b12a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_sequences_for_multivariate(X, y, window_size):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(window_size, len(X)):\n",
    "        # On prend une fen√™tre de 'window_size' jours pour X\n",
    "        X_seq.append(X[i-window_size:i])\n",
    "        # La cible est la valeur de y juste apr√®s cette fen√™tre\n",
    "        y_seq.append(y[i])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "#Param√®tre critique : quelle longueur d'historicit√© nous voulons ? Ici ce param√®tre est r√©gl√© √† 30, mais c'est potentiellement beaucoup trop long\n",
    "window_size = 30 \n",
    "\n",
    "X_train_multi_scaled_seq, y_train_multi_scaled_seq = create_sequences_for_multivariate(X_train_multi_scaled, y_train_multi_scaled, window_size)\n",
    "X_test_multi_scaled_seq, y_test_multi_scaled_seq = create_sequences_for_multivariate(X_test_multi_scaled, y_test_multi_scaled, window_size)\n",
    "\n",
    "\n",
    "print(f\"Forme de dataset_train : {dataset_train.shape}\")\n",
    "print(f\"Forme de dataset_test : {dataset_test.shape}\") \n",
    "\n",
    "print(f\"Forme de X_train_scaled_multi : {X_train_multi.shape}\")\n",
    "print(f\"Forme de y_train_scaled_multi : {y_train_multi.shape}\")\n",
    "\n",
    "print(f\"Forme de X_train_multi_scaled_seq : {X_train_multi_scaled_seq.shape}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61198b19",
   "metadata": {},
   "source": [
    "## Section 3 : cr√©ation et entrainement du mod√®le LSTM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0251bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Initialisation du mod√®le\n",
    "model_lstm_multi = Sequential([\n",
    "   \n",
    "    # input_shape = (nb_pas_de_temps, nb_features)\n",
    "    #######################################\n",
    "    #ligne √† remplir\n",
    "    #######################################\n",
    "    LSTM(50, activation='relu', input_shape=(,)),\n",
    "    \n",
    "    \n",
    "    # Dropout pour √©viter le sur-apprentissage (overfitting)\n",
    "    Dropout(0.2),   \n",
    "    \n",
    "    # Couche de sortie : 1 neurone pour la pr√©diction finale (valeur continue)\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compilation\n",
    "model_lstm_multi.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "# 3. Entra√Ænement\n",
    "# epochs : nombre de passages sur les donn√©es\n",
    "# batch_size : nombre d'√©chantillons trait√©s avant la mise √† jour des poids\n",
    "history = model_lstm_multi.fit(\n",
    "    #X_train_multi_scaled_seq[:,:,0], y_train_multi_scaled_seq, \n",
    "    X_train_multi_scaled_seq,y_train_multi_scaled_seq,\n",
    "    epochs=1000, \n",
    "    batch_size=32, \n",
    "    validation_split=0.1, # On garde 10% pour valider pendant l'entra√Ænement\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7649d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Pr√©dictions\n",
    "predictions_scaled = model_lstm_multi.predict(X_test_multi_scaled_seq)\n",
    "\n",
    "# 2. D√©normalisation des pr√©dictions\n",
    "y_pred = scaler_multi_y.inverse_transform(predictions_scaled)\n",
    "y_actual = scaler_multi_y.inverse_transform(y_test_multi_scaled_seq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210da81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(y_actual, label='Consommation R√©elle (MW)', color='blue', alpha=0.7)\n",
    "plt.plot(y_pred, label='Pr√©diction LSTM (MW)', color='red', linestyle='--')\n",
    "\n",
    "plt.title('Pr√©diction de la consommation √©lectrique √† Brest (Multivari√© : M√©t√©o + Lags)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Puissance (MW)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8462e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# 1. MAE : Mean Absolute Error (Erreur Absolue Moyenne)\n",
    "#######################################\n",
    "#ligne √† remplir\n",
    "#######################################\n",
    "mae = mean_absolute_error()\n",
    "\n",
    "# 2. RMSE : Root Mean Squared Error (Erreur Quadratique Moyenne)\n",
    "#######################################\n",
    "#ligne √† remplir\n",
    "#######################################\n",
    "rmse = np.sqrt(mean_squared_error())\n",
    "\n",
    "# 3. MAPE : Mean Absolute Percentage Error (Erreur en Pourcentage)\n",
    "#######################################\n",
    "#ligne √† remplir\n",
    "#######################################\n",
    "mape = np.mean(np.abs(() / )) * 100\n",
    "\n",
    "print(f\"üìä Performances du mod√®le LSTM :\")\n",
    "print(f\"MAE  : {mae:.2f} MW\")\n",
    "print(f\"RMSE : {rmse:.2f} MW\")\n",
    "print(f\"MAPE : {mape:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59e8fd5",
   "metadata": {},
   "source": [
    "## Test du mod√®le de fondation MOIRAI ##\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "Les mod√®les de fondations, tr√®s connus pour le traitement d'images ou pour le NLP, arrivent depuis peu de temps pour les donn√©es tabulaires et les time series. La promesse : faire des pr√©dictions en zero-shot learning, sans fine tuning sur nos propres data, avec seulement des donn√©es de \"contexte\".\n",
    "\n",
    "Cette approche est √©mergente, les librairies sont encore mouvantes, pas simple d'arriver √† les faire fonctionner.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "Nous utilisons ici MOIRAI, un mod√®le de fondation √† l'√©tat de l'art, voyons ce qu'il est capable de faire ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117cf665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "import uni2ts.model.moirai as moirai\n",
    "from uni2ts.model.moirai import MoiraiModule, MoiraiForecast\n",
    "# 1. S√©lection des donne√©s utilis√©es comme historique (ou contexte) par MOIRAI, soit les 200 jours avant le mois de d√©cembre 2025\n",
    "data_for_prediction = dataset_test.iloc[-230:-30].copy()\n",
    "data_for_prediction.index = pd.to_datetime(data_for_prediction.index)\n",
    "\n",
    "#Explicitation de la nature journali√®re des donn√©es\n",
    "base_freq = \"D\"\n",
    "data_for_prediction = data_for_prediction.asfreq(base_freq)\n",
    "\n",
    "data_for_prediction = data_for_prediction.ffill()\n",
    "\n",
    "# 2. Preparation du dataset au format GluonTS\n",
    "# Ici, la date est l'index\n",
    "dataset = PandasDataset(\n",
    "    data_for_prediction,\n",
    "    target=\"consommation\",\n",
    "    timestamp=None,\n",
    "    freq=base_freq,\n",
    "    feat_dynamic_real=features\n",
    ")\n",
    "\n",
    "# 3. Longueurs de pr√©diction et de contexte\n",
    "requested_prediction_length = 30\n",
    "context_length = 150\n",
    "\n",
    "max_pred = len(data_for_prediction) - context_length\n",
    "if max_pred <= 0:\n",
    "    raise ValueError(\n",
    "        f\"Pas assez de donnees: len(data_for_prediction)={len(data_for_prediction)} \"\n",
    "        f\"< context_length={context_length}.\"\n",
    "    )\n",
    "\n",
    "prediction_length = min(requested_prediction_length, max_pred)\n",
    "\n",
    "\n",
    "# On charge ici le mod√®le small, si vous avez une bonne bande passante vous pouvez charger le mod√®le large!\n",
    "module = MoiraiModule.from_pretrained(\"Salesforce/moirai-1.0-R-small\")\n",
    "\n",
    "predictor = MoiraiForecast(\n",
    "    module=module,\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=context_length,\n",
    "    patch_size=\"auto\",\n",
    "    num_samples=100,\n",
    "    target_dim=1,\n",
    "    feat_dynamic_real_dim=len(features),\n",
    "    past_feat_dynamic_real_dim=0\n",
    ")\n",
    "\n",
    "final_predictor = predictor.create_predictor(batch_size=32)\n",
    "forecast_it = final_predictor.predict(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e025e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. On recupere le premier (et seul) forecast de notre liste\n",
    "forecasts = list(forecast_it)\n",
    "\n",
    "if len(forecasts) == 0:\n",
    "    raise ValueError(\n",
    "        \"Aucun forecast renvoy√©.\"\n",
    "    )\n",
    "forecast_entry = forecasts[0]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "\n",
    "forecast_index = forecast_entry.index\n",
    "\n",
    "# Normalise les types d'index (Period -> Timestamp) et les timezones\n",
    "if isinstance(forecast_index, pd.PeriodIndex):\n",
    "    forecast_index = forecast_index.to_timestamp()\n",
    "\n",
    "series = dataset_test['consommation']\n",
    "\n",
    "\n",
    "# Historique: on prend juste avant le debut de la prediction\n",
    "history_end = forecast_index[0]\n",
    "history = series.loc[:history_end].iloc[-100:]\n",
    "plt.plot(history.index, history.values, label=\"Historique (MW)\", color=\"black\", linewidth=1.5)\n",
    "\n",
    "actual = series.reindex(forecast_index)\n",
    "plt.plot(actual.index, actual.values, label=\"Realite (Ground Truth)\", color=\"red\", linestyle=\"--\", linewidth=2)\n",
    "\n",
    "# 3. Trace de la mediane + intervalles\n",
    "q50 = forecast_entry.quantile(0.5)\n",
    "q90 = forecast_entry.quantile(0.9)\n",
    "q10 = forecast_entry.quantile(0.1)\n",
    "\n",
    "plt.plot(forecast_index, q50, color='g', label=\"Prediction MOIRAI (Mediane)\")\n",
    "plt.fill_between(forecast_index, q10, q90, color='g', alpha=0.2, label=\"Intervalle 10-90%\")\n",
    "\n",
    "plt.title(\"Prediction de Consommation electrique : MOIRAI (Zero-Shot)\", fontsize=14)\n",
    "plt.ylabel(\"Puissance (MW)\")\n",
    "plt.xlabel(\"Temps\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
