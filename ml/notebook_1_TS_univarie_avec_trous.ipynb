{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94bf1699",
   "metadata": {},
   "source": [
    "##############################################################################\n",
    "##############################################################################\n",
    "### **Atelier \"Faire du Machine Learning et du Deep Learning sur des Time Series\"**\n",
    "##############################################################################\n",
    "##############################################################################\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "#### **Partie 1 : Machine Learning sur des Times Series univariées** \n",
    "\n",
    "Dans cette première partie, nous allons explorer les différentes méthodes  de ML pour les Time Series sur des séries univariées, en mettant en évidence leurs faiblesses et les résolutions proposées par les méthodes suivantes.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "Méthodes explorées :\n",
    "\n",
    "1 : ARIMA\n",
    "\n",
    "2 : SARIMA\n",
    "\n",
    "3 : TBATS\n",
    "\n",
    "4 : LSTM\n",
    "\n",
    "############################################################\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Objectif de l'Atelier : prédire la consommation éléctrique de la métropole de Brest\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b2fced",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Section 0 : récupération  et mise en forme des données ##\n",
    "\n",
    "Nous allons récupérer les informations historiques de consommation électrique en MW de la métropole de Brest. Pour ceci, nous allons faire appel à l'API de l'ODRE et récupérer les informations de consommation pour la zone géographique choisie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a40145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def fetch_brest_electricity_data():\n",
    "    base_url = \"https://odre.opendatasoft.com/api/explore/v2.1/catalog/datasets/eco2mix-metropoles-tr/exports/json\"\n",
    "    \n",
    "    # Paramètres de la requête\n",
    "    params = {\n",
    "        \"where\": \"libelle_metropole='Brest Métropole' AND date_heure >= '2020-01-01' AND date_heure <= '2025-12-31'\",\n",
    "        \"order_by\": \"date_heure ASC\",\n",
    "        \"timezone\": \"UTC\"\n",
    "    }\n",
    "    \n",
    "    print(\"Récupération des données pour Brest Métropole (2020-2025)...\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        df = pd.DataFrame(data)\n",
    "        print(df.head())\n",
    "        \n",
    "        # On garde ce qui nous faut pour l'atelier : le temps et la consommation électrique associée (puissance en MW)\n",
    "        df = df[['date_heure', 'consommation']].dropna()\n",
    "        \n",
    "        print(f\"Chargement Terminé ! {len(df)} lignes récupérées.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Erreur lors de la récupération : {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "df_conso_brest = fetch_brest_electricity_data()\n",
    "\n",
    "if df_conso_brest is not None:\n",
    "    print(df_conso_brest.head())\n",
    "    # Sauvegarde pour l'atelier\n",
    "    df_conso_brest.to_csv(\"conso_brest_2020_2025.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad75fca",
   "metadata": {},
   "source": [
    "### traitement initial des séries temporelles ###\n",
    "\n",
    "Les séries temporelles ont besoin de traitements spécifiques.\n",
    "\n",
    "Dans le cas présent, nous allons préparer les données en transformant le dataframe en série temporelle, grâce à l'ajout d'un index temporel, très pratique pour manipuler les datas par date.\n",
    "\n",
    "De plus, nous allons changer la temporalité des données en les rendant journalières, car c'est l'échelle temporelle de notre analyse.\n",
    "\n",
    "Enfin, nous allons définir les jeux d'entrainement et de test. Dans ce cas, nous allons faire apprendre nos modèles sur la période 2020-2024, et évaluer sur l'année 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3274b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# indexation temporelle\n",
    "df_conso_brest['date_heure'] = pd.to_datetime(df_conso_brest['date_heure'], utc=True)\n",
    "df_conso_brest = df_conso_brest.set_index('date_heure').sort_index()\n",
    "\n",
    "# changement de temporalité des données\n",
    "#######################################\n",
    "#ligne à remplir\n",
    "#######################################\n",
    "df_daily = df_conso_brest['consommation'].resample(\n",
    "\n",
    "# Définition des périodes pour les jeux d'apprentissage\n",
    "dataset_train = df_daily['2015-01-01':'2024-12-31']\n",
    "dataset_test = df_daily['2025-01-01':'2025-12-31']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5ac228",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "A vous de jouer ! complétez la fonction Resample pour transformer les données en données journalières\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4a0daa",
   "metadata": {},
   "source": [
    "## SECTION 1 : Exploration des données ## \n",
    "\n",
    "Dans cette section, nous allons chercher à comprendre la nature des séries temporelles en les décomposant, en analysant leur stationnarité et en recherchant des saisonnalités\n",
    "\n",
    "### SECTION 1.1 : Visualisation de la décomposition ### \n",
    "L'analyse commence par une décomposition classique (STL ou additive) afin d'isoler les composantes fondamentales du signal :\n",
    "\n",
    "    La Tendance (Trend) : Capture l'évolution à long terme de la consommation électrique.\n",
    "\n",
    "    La Saisonnalité (Seasonal) : Identifie les variations cycliques périodiques (ici fixée sur 12 unités).\n",
    "    \n",
    "    Le Résidu (Residual/Noise) : Représente la composante stochastique ou les anomalies non expliquées par les facteurs déterministes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78381507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposition = seasonal_decompose(dataset_train, model='additive')\n",
    "\n",
    "decomposition.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b90c21",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "A partir de cette décomposition, cherchez à comprendre qualitativement :\n",
    "- la saisonnalité de vos données\n",
    "- les pics observés en hiver\n",
    "- quelle période de l'année est le plus sujette aux fluctuations aléatoires de consommation\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b06cd5",
   "metadata": {},
   "source": [
    "### Section 1.2 : Test de stationnarité ## \n",
    "\n",
    "La validité des modèles de type Box-Jenkins repose sur la stationnarité de la série (moyenne et variance constantes dans le temps).\n",
    "\n",
    "    Test de Dickey-Fuller Augmenté (ADF) : Nous testons l'hypothèse nulle (H0​) de présence d'une racine unitaire. Une p-value ≤0.05 indique une série stationnaire.\n",
    "\n",
    "\n",
    "#### Pourquoi différencier une série non stationnaire ? #### \n",
    "\n",
    "Les modèles de ML sont excellents pour apprendre des relations entre des variables, mais ils sont très mauvais pour extrapoler.\n",
    "\n",
    "    Le problème de la dérive : un modèle ne pourra pas prédire correctement des valeurs qu'il n'aura pas vu lors de l'entrainement\n",
    "\n",
    "    La stationnarité : En calculant la différence, la nouvelle série qui oscille autour de zéro. ce qui est beaucoup plus stable et facile à apprendre pour lui.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4ca158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def check_stationarity(series):\n",
    "    result = adfuller(series.dropna())\n",
    "    print(f'ADF Statistic: {result[0]:.3f}')\n",
    "    print(f'p-value: {result[1]:.3e}')\n",
    "    if result[1] <= 0.05:\n",
    "        print(\"=> La série est stationnaire (Rejet de H0)\")\n",
    "    else:\n",
    "        print(\"=> La série n'est pas stationnaire (Echec de rejet de H0)\")\n",
    "\n",
    "\n",
    "print(\"--- Série Brute ---\")\n",
    "check_stationarity(dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d91bf3",
   "metadata": {},
   "source": [
    "### Section 1.3 : Analyse des saisonnalités ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc26195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# ACF : Aide à identifier le terme MA (q)\n",
    "plot_acf(dataset_train.dropna(),  ax=ax1)\n",
    "ax1.set_title(\"Autocorrélation (ACF)\")\n",
    "\n",
    "# PACF : Aide à identifier le terme AR (p)\n",
    "plot_pacf(dataset_train.dropna(),  ax=ax2)\n",
    "ax2.set_title(\"Autocorrélation Partielle (PACF)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321f8b54",
   "metadata": {},
   "source": [
    "<br>\n",
    "L'interprétation des ACF et des PACF est toujours un moment compliqué. Dans notre cas, nous pouvons dire que les observations des 2 jours précédents sont importants, et qu'il y a une saisonnalité de 7 qui semble apparaitre. De plus , l'ACF voit les intensités diminuer au fil du temps, c'est signe d'une non-stationnarité\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "Conclusion des analyses : la TS a besoin d'une différentation pour devenir stationnaire. Testons donc cette conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd01287",
   "metadata": {},
   "source": [
    "### Section 1.4 : Stationnarisation de la Time Series ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d0b331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def check_stationarity(series):\n",
    "    result = adfuller(series.dropna())\n",
    "    print(f'ADF Statistic: {result[0]:.3f}')\n",
    "    print(f'p-value: {result[1]:.3e}')\n",
    "    if result[1] <= 0.05:\n",
    "        print(\"=> La série est stationnaire (Rejet de H0)\")\n",
    "    else:\n",
    "        print(\"=> La série n'est pas stationnaire (Echec de rejet de H0)\")\n",
    "\n",
    "\n",
    "print(\"--- Série Brute ---\")\n",
    "check_stationarity(dataset_train)\n",
    "\n",
    "#######################################\n",
    "#ligne à remplir\n",
    "#######################################\n",
    "train_diff = dataset_train.\n",
    "\n",
    "print(\"\\n--- Série après différenciation une différentiation ---\")\n",
    "check_stationarity(train_diff)\n",
    "\n",
    "# 4. Visualisation pédagogique\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(dataset_train)\n",
    "plt.title('Série Originale (Non-stationnaire)')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(train_diff, color='orange')\n",
    "plt.title('Série Différenciée ')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96e0d2f",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "A partir de cette analyse, dressez le tableau qualitatif et quantitatif de votre TS.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1eb0c5",
   "metadata": {},
   "source": [
    "## SECTION 2  : Création des modèles de ML ##\n",
    "### SECTION 2.1  : Modélisation autorégressive intégrée (ARIMA) ### \n",
    "\n",
    "Nous implémentons un modèle ARIMA comme modèle de référence univariée. Ce modèle combine :\n",
    "\n",
    "    AR (Auto-regressive) : La dépendance entre une observation et un certain nombre d'observations décalées (lag).\n",
    "\n",
    "    I (Integrated) : L'utilisation de la différenciation des données brutes pour rendre la série stationnaire.\n",
    "\n",
    "    MA (Moving Average) : La dépendance entre une observation et l'erreur résiduelle d'un modèle de moyenne mobile appliqué aux observations décalées.\n",
    "\n",
    "\n",
    "ARIMA demande 3 paramètres :\n",
    "- p : le nombre de valeurs précedentes à utiliser pour l'AR : d'après l'analyse précedente, il est ici de \"Remplissez la valeur\"\n",
    "- d : le nombre de différentiation à effectuer pour que la TS devienne stationnaire, il est ici de \"Remplissez la valeur\"\n",
    "- q : le nombre de d'erreurs sur les valeurs précedentes à utiliser pour le MA, , il est ici de \"Remplissez la valeur\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5283dc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "#######################################\n",
    "#ligne à remplir\n",
    "#######################################\n",
    "model_arima = ARIMA(dataset_train, order=(, , ))\n",
    "model_arima_fit = model_arima.fit()\n",
    "\n",
    "# Prédiction sur 2025\n",
    "forecast_arima = model_arima_fit.forecast(steps=len(dataset_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815923bb",
   "metadata": {},
   "source": [
    "On affiche ici le graphique de la prédiction par rapport à la réalité. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc46160",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(dataset_test.index, dataset_test, label='Réel 2025', color='black', alpha=0.5)\n",
    "plt.plot(dataset_test.index, forecast_arima, label='prédiction ARIMA ', color='blue')\n",
    "\n",
    "plt.title(\"prédictions de la consommation en 2025 avec ARIMA\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6898e0e7",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "Question : Pouvez-vous expliquer pourquoi la méthode ARIMA ne fonctionne pas sur nos données ?\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5f2544",
   "metadata": {},
   "source": [
    "### SECTION 2.2 : Modélisation SARIMA (Saisonal ARIMA) ###\n",
    "\n",
    "Le modèle SARIMA ​ étend ARIMA en intégrant des paramètres saisonniers explicites. Il permet de modéliser les corrélations à intervalles réguliers, crucial pour les données de consommation présentant par exemple des motifs hebdomadaires distincts (jours ouvrés vs week-end)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba8016f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Définition du modèle\n",
    "# order=(p, d, q) \n",
    "# seasonal_order=(P, D, Q, s) \n",
    "#######################################\n",
    "#ligne à remplir\n",
    "#######################################\n",
    "model_sarima = SARIMAX(dataset_train, \n",
    "                order=(),              \n",
    "                seasonal_order=(), \n",
    "                enforce_stationarity=False,\n",
    "                enforce_invertibility=False)\n",
    "\n",
    "# Entraînement\n",
    "model_sarima_fit = model_sarima.fit(disp=False)\n",
    "\n",
    "forecast_sarima = model_sarima_fit.forecast(steps=len(dataset_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6532ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(dataset_test.index, dataset_test, label='Réel 2025', color='black', alpha=0.5)\n",
    "plt.plot(dataset_test.index, forecast_sarima, label='Prédiction SARIMA', color='blue')\n",
    "\n",
    "plt.title(\"prédictions 2025 avec SARIMA\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a34a154",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "A votre avis, pourquoi SARIMA ne fonctionne pas ? quel élément n'est pas pris en compte ?\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac206f",
   "metadata": {},
   "source": [
    "### SECTION 2.3  Modélisation TBATS (Saisonnalités Multiples) ###\n",
    "\n",
    "Le modèle TBATS (Trigonometric, Box-Cox, ARMA, Trend, Seasonal) est une approche avancée utilisant des séries de Fourier pour modéliser des saisonnalités multiples et complexes. Ici, il traite simultanément :\n",
    "\n",
    "    La saisonnalité hebdomadaire (s1​=7).\n",
    "\n",
    "    La saisonnalité annuelle (s2​=365.25). Il inclut également une transformation de Box-Cox pour gérer l'hétéroscédasticité du signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41edb3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tbats import TBATS\n",
    "\n",
    "\n",
    "\n",
    "# 1. Initialisation du modèle\n",
    "# On définit explicitement les deux cycles : 7 jours et 365.25 jours\n",
    "#######################################\n",
    "#ligne à remplir\n",
    "#######################################\n",
    "estimator = TBATS(seasonal_periods=)\n",
    "\n",
    "# 2. Entraînement\n",
    "model_tbats = estimator.fit(dataset_train)\n",
    "\n",
    "# 3. Prévision\n",
    "forecast_tbats = model_tbats.forecast(steps=len(dataset_test))\n",
    "\n",
    "\n",
    "forecast_index = pd.date_range(start=dataset_train.index[-1], periods=len(dataset_test) + 1, freq='D')[1:]\n",
    "forecast_series = pd.Series(forecast_tbats, index=forecast_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8847175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "\n",
    "plt.plot(dataset_test.index, dataset_test, color='gray', alpha=0.5, label='Réel 2025')\n",
    "plt.plot(dataset_test.index, forecast_sarima, color='blue', linestyle='--', label='SARIMA (S=7)')\n",
    "plt.plot(dataset_test.index, forecast_tbats, color='red', linewidth=2, label='TBATS (S=7 + S=365)')\n",
    "\n",
    "plt.title('SARIMA vs TBATS : Impact de la double saisonnalité', fontsize=14)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Valeur')\n",
    "plt.legend()\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b48233",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "Maintenant que les 3 méthodes de ML sont implémentées, vous pouvez retracer chaque amélioration apportée pour prendre en compte la complexité des Time Series\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de90ca06",
   "metadata": {},
   "source": [
    "### SECTION 2.4 : Calcul des erreurs de prédiction des 3 méthodes de ML ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c618785",
   "metadata": {},
   "source": [
    "Nous allons maintenant calculer l'erreur de prédiction de chaque approche.\n",
    "Nous calculerons l'erreur sur l'année entière, puis l'erreur mensuelle, et enfin l'erreur hebdomadaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06362571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Préparation d'un DataFrame de comparaison\n",
    "results = pd.DataFrame({\n",
    "    'Réel': dataset_test,\n",
    "    'SARIMA': forecast_sarima,\n",
    "    'TBATS': forecast_tbats\n",
    "}, index=dataset_test.index)\n",
    "\n",
    "# 2. Fonction de calcul des métriques\n",
    "#######################################\n",
    "#ligne à remplir\n",
    "#######################################\n",
    "def get_metrics(df, actual_col, pred_col):\n",
    "    mae = \n",
    "    rmse = \n",
    "    return pd.Series({'MAE': mae, 'RMSE': rmse})\n",
    "\n",
    "# --- A. Calculs des métriques sur tout le dataset ---\n",
    "print(\"--- Métriques Globales ---\")\n",
    "global_sarima = get_metrics(results, 'Réel', 'SARIMA')\n",
    "global_tbats = get_metrics(results, 'Réel', 'TBATS')\n",
    "print(f\"SARIMA : MAE={global_sarima['MAE']:.2f}, RMSE={global_sarima['RMSE']:.2f}\")\n",
    "print(f\"TBATS  : MAE={global_tbats['MAE']:.2f}, RMSE={global_tbats['RMSE']:.2f}\")\n",
    "\n",
    "# --- B. Métriques MENSUELLES ---\n",
    "# On groupe par mois pour voir où les modèles échouent\n",
    "monthly_errors = results.groupby(results.index.month).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'MAE_SARIMA': mean_absolute_error(x['Réel'], x['SARIMA']),\n",
    "        'MAE_TBATS': mean_absolute_error(x['Réel'], x['TBATS'])\n",
    "    })\n",
    ")\n",
    "\n",
    "# --- C. Métriques HEBDOMADAIRES ---\n",
    "weekly_errors = results.groupby(results.index.isocalendar().week).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'MAE_SARIMA': mean_absolute_error(x['Réel'], x['SARIMA']),\n",
    "        'MAE_TBATS': mean_absolute_error(x['Réel'], x['TBATS'])\n",
    "    })\n",
    ")\n",
    "\n",
    "# Affichage des premières lignes mensuelles\n",
    "print(\"\\n--- Erreurs Mensuelles ---\")\n",
    "print(monthly_errors)\n",
    "print(\"\\n--- Erreurs hebdomadaires ---\")\n",
    "print(weekly_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75748bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = monthly_errors.index\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "\n",
    "rects1 = ax.bar(x - width/2, monthly_errors['MAE_SARIMA'], width, \n",
    "                label='MAE SARIMA', color='royalblue', alpha=0.7)\n",
    "\n",
    "\n",
    "rects2 = ax.bar(x + width/2, monthly_errors['MAE_TBATS'], width, \n",
    "                label='MAE TBATS', color='crimson', alpha=0.7)\n",
    "\n",
    "\n",
    "ax.set_ylabel('Erreur Moyenne (MAE)')\n",
    "ax.set_title('Comparaison de l\\'erreur par mois : SARIMA vs TBATS')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Jan', 'Fév', 'Mar', 'Avr', 'Mai', 'Juin', 'Juil', 'Août', 'Sep', 'Oct', 'Nov', 'Déc'])\n",
    "ax.legend()\n",
    "\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6905a2",
   "metadata": {},
   "source": [
    "### SECTION 2.5 : Modélisation profonde par LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72bb4ea",
   "metadata": {},
   "source": [
    "Les modèles de Machine Learning (et particulièrement les réseaux de neurones ou SVR) sont sensibles à l'échelle des données, il va donc falloir normaliser les données\n",
    "\n",
    "    MinMaxScaler / StandardScaler : Transformation des valeurs pour les ramener dans un intervalle réduit (ex: [0,1]).\n",
    "\n",
    "    Objectif : Éviter que la variable \"Consommation\" (en centaines de MW) n'écrase par exemple la variable \"Température\" (en dizaines de degrés)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da57224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1. Normalisation\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "train_scaled = scaler.fit_transform(dataset_train.values.reshape(-1, 1))\n",
    "test_scaled = scaler.transform(dataset_test.values.reshape(-1, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf84c3e",
   "metadata": {},
   "source": [
    "Nous allons maintenant créer les séquences qui serviront au LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3a0b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fonction pour créer les séquences\n",
    "def create_sequences(data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(n_steps, len(data)):\n",
    "        X.append(data[i-n_steps:i, 0])\n",
    "        y.append(data[i, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "#Paramètre critique : quelle longueur d'historicité nous voulons ? Ici ce paramètre est réglé à 30, mais c'est potentiellement beaucoup trop long\n",
    "n_steps = 30 \n",
    "\n",
    "X_train, y_train = create_sequences(train_scaled, n_steps)\n",
    "X_test, y_test = create_sequences(test_scaled, n_steps)\n",
    "\n",
    "# 4. Reshape pour le LSTM [samples, time steps, features]\n",
    "# Le LSTM attend une entrée en 3D\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "print(f\"Forme de X_train : {X_train.shape}\") # (Nb_jours, 30, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e53222a",
   "metadata": {},
   "source": [
    "Nous allons maintenant entrainer le LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025c3006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ---------------------------------------\n",
    "# TensorBoard : définition du dossier de logs\n",
    "# ---------------------------------------\n",
    "#log_dir = os.path.join(\"logs\", \"fit\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "#tb_callback = TensorBoard(\n",
    "#    log_dir=log_dir,\n",
    "#    histogram_freq=1,      # histogrammes des poids (1 = à chaque epoch)\n",
    "#    write_graph=True,      # graphe du modèle\n",
    "#    write_images=False,\n",
    "#    update_freq=\"epoch\",\n",
    "#    profile_batch=0        # désactive le profiler par défaut (évite overhead)\n",
    "#)\n",
    "\n",
    "\n",
    "# 1. Architecture du modèle\n",
    "model_lstm = Sequential([\n",
    "    # input_shape = (nombre de jours regardés, nombre de variables)\n",
    "    #######################################\n",
    "    #ligne à remplir\n",
    "    #######################################\n",
    "    LSTM(units=50, activation='tanh', input_shape=()),\n",
    "    \n",
    "    # Dropout : aide à prévenir le surapprentissage en \"désactivant\" 20% des neurones aléatoirement\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    # Couche de sortie : 1 neurone pour prédire la valeur du jour suivant\n",
    "    Dense(units=1)\n",
    "])\n",
    "\n",
    "# 2. Compilation\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "#######################################\n",
    "#ligne à remplir\n",
    "#######################################\n",
    "early_stop = EarlyStopping(monitor=, patience=, restore_best_weights)\n",
    "# 3. Entraînement\n",
    "\n",
    "history = model_lstm.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop], \n",
    "    #    callbacks=[early_stop, tb_callback], \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "weights = model_lstm.get_weights()\n",
    "with open(\"modele_lstm_univarie.pkl\", \"wb\") as f:\n",
    "    pickle.dump(weights, f)\n",
    "\n",
    "#print(f\"Logs TensorBoard écrits dans : {log_dir}\")\n",
    "\n",
    "#visualisation de Tensorboad par ligne de commande\n",
    "#python -m tensorboard.main --logdir logs/fit --port 6006 --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f379b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prédiction sur les données de test\n",
    "predictions_scaled = model_lstm.predict(X_test)\n",
    "\n",
    "# 2. Inversion de la normalisation (Denormalization)\n",
    "predictions_unscaled = scaler.inverse_transform(predictions_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# 3. Création d'une série pandas pour l'affichage\n",
    "forecast_index_lstm = dataset_test.index[n_steps:]\n",
    "forecast_lstm = pd.Series(predictions_unscaled.flatten(), index=forecast_index_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a797ecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "\n",
    "plt.plot(dataset_test.index, dataset_test, color='gray', alpha=0.5, label='Réel 2025')\n",
    "plt.plot(dataset_test.index, forecast_sarima, color='blue', linestyle='--', label='SARIMA (S=7)')\n",
    "plt.plot(dataset_test.index, forecast_tbats, color='red', linewidth=2, label='TBATS (S=7 + S=365)')\n",
    "plt.plot(forecast_lstm.index, forecast_lstm, color='green', linewidth=2, label='LSTM')\n",
    "\n",
    "plt.title('SARIMA vs TBATS vs LSTM', fontsize=14)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Valeur')\n",
    "plt.legend()\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735f820e",
   "metadata": {},
   "source": [
    "### SECTION 2.6 : Calcul des erreurs de prédiction de LSTM et comparaison avec les 3 méthodes de ML ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30efe3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_lstm = mean_absolute_error()\n",
    "rmse_lstm = np.sqrt(mean_squared_error()\n",
    "\n",
    "\n",
    "# --- A. Métriques GLOBALES (365 jours) ---\n",
    "print(\"--- Métriques Globales ---\")\n",
    "global_sarima = get_metrics(results, 'Réel', 'SARIMA')\n",
    "global_tbats = get_metrics(results, 'Réel', 'TBATS')\n",
    "print(f\"SARIMA : MAE={global_sarima['MAE']:.2f} MW, RMSE={global_sarima['RMSE']:.2f} MW\")\n",
    "print(f\"TBATS  : MAE={global_tbats['MAE']:.2f} MW, RMSE={global_tbats['RMSE']:.2f} MW\")\n",
    "print(f\"LSTM  : MAE={mae_lstm:.2f} MW, RMSE={rmse_lstm:.2f} MW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f27f1c",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "A vous de jouer !\n",
    "\n",
    "Améliorez les hyper-paramètres du LSTM pour améliorer la capacité prédictive. Vous pouvez changer la longueur des séquences, l'architecture du LSTM (Attention au surapprentissage!)\n",
    "<br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
